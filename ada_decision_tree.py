# -*- coding: utf-8 -*-
"""ADA - Decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RQhbv7223LPSyr6T7eW7TxnyYlpBuC8
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import xgboost
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

wine = pd.read_csv('jakoscWina.csv', sep=',')
wine.head()

wine.columns

print ("Size of train data : {}" .format(wine.shape))

wine.isnull().sum()

#brak warości zerowych

wine.info()

#quality ma format int64, którego nie trzeba zmieniać do podziału na 0 i 1

wine.drop_duplicates(inplace=True)
print(wine)

#zmiana tekstowych wartości na binarne w Quality

wine['quality'].value_counts()

wine['quality'] = wine['quality'].apply(lambda x : 1 if(x>6.5) else 0)

wine

#wykresy

import plotly.express as px
from matplotlib.font_manager import FontProperties
wine_new = wine['quality'].value_counts().rename_axis('Winequality').reset_index(name='counts')
wine_new
fig = px.pie(wine_new, values='counts', names='Winequality')
fig.show()

#wykres obrazuje rozkład na jakość win, które mają warości powyżej 6,5 - czerwone 13.5% oraz te które mają mniej niż 6,5 - niebieskie 86.5%

plt.figure(figsize=(16,10))
sns.scatterplot(x='alcohol',y='residual sugar',data=wine,hue='quality')
plt.yticks(rotation =90,fontsize=12)
plt.xticks(fontsize=12)
plt.title("Wykres",fontsize=30)
plt.xlabel('Alchohol',fontsize=26)
plt.ylabel('Residual Sugar',fontsize=26)
plt.show()

#wpłay residual sugar oraz alchohol na jakość (quality) wina

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(16,10))
sns.boxplot(y='fixed acidity',x='quality',data=wine)
plt.yticks(rotation =90,fontsize=12)
plt.xticks(fontsize=12)
plt.title("Impact of fixed acidity on Quality",fontsize=30)
plt.xlabel('Quality',fontsize=26)
plt.ylabel('Fixed Acidity',fontsize=26)
plt.show()

# wykres obrazuje wpływ fixed acidity na jakość (quality) wina

plt.figure(figsize=(16,10))
sns.boxplot(y='residual sugar',x='quality',data=wine)
plt.yticks(rotation =90,fontsize=12)
plt.xticks(fontsize=12)
plt.title("PLOT- B Checking outliers",fontsize=30)
plt.xlabel('Quality',fontsize=26)
plt.ylabel('Residual Sugar',fontsize=26)
plt.show()

plt.figure(figsize=(16,10))
sns.boxplot(y='total sulfur dioxide',x='quality',data=wine)
plt.yticks(rotation =90,fontsize=12)
plt.xticks(fontsize=12)
plt.title("PLOT- D Checking outliers",fontsize=30)
plt.xlabel('Quality',fontsize=26)
plt.ylabel('Total Sulphur Dioxide',fontsize=26)
plt.show()

#powyższe 2 wykresu obrazują występowanie outliers pomiędzy quality a total sulphur dioxide oraz residual sugar

sns.heatmap(wine.corr())

#mapa pokazuje korelacje pomiędzy poszczególnymi wartościami - im jaśniejsza, tym korelacja większa.
#Na przykład mapa pokazuje, że korealcje pomiędzy ph a citric acid jest niska, a pomiędzy fixed acidity a density w miarę wysoka

wine['alcohol'].hist()

# Na histiogramie są zaprezentowane w jakich proporcjach jest rozkład alkoholu w winie

#przygotowanie danych

X = wine.drop(['quality'],axis=1)
Y = wine['quality']
del wine

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.20,random_state = 90)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

from sklearn.model_selection import RandomizedSearchCV
# Liczba drzew
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Liczba cech przy każdym podziale
max_features = ['auto', 'sqrt']
#Kryteria
criterion = ['gini','entropy']
# Max liczba liści 
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Min liczba przykładów potrzebnych do podziału węzłów
min_samples_split = [2, 5, 10]
# Min liczba przykładów potrzebnych dla każdego liścia 
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'criterion': criterion,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf_tune = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf_tune,
                               param_distributions = random_grid,
                               n_iter = 100,
                               cv = 3,
                               verbose=2,
                               random_state=42,
                               n_jobs = -1)
rf_random.fit(X_train,Y_train)

rf_random.best_params_

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(rf_random,X_test,Y_test)

Y_pred = rf_random.predict(X_test)

accuracy_score(Y_test,Y_pred)

#dokładność modelu wynosi ~89%

f1_score(Y_test,Y_pred)

#Model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101);
print(f'Shape of the X_train: {X_train.shape}');
print(f'Shape of the X_test: {X_test.shape}');
print(f'Shape of the y_train: {y_train.shape}');
print(f'Shape of the y_test: {y_test.shape}');

def model_evaluation(model, X_train, y_train, X_test, y_test):
    print('Starting ...')
    ss = StandardScaler()
    X_train_ss = ss.fit_transform(X_train)
    X_test_ss = ss.fit_transform(X_test)
    print("Scaling process is done ...")
    mod = model.fit(X_train, y_train)
    mod_pred = model.predict(X_test)
    print("Classification report of the Model: \n {}".format(classification_report(y_test, mod_pred)))
    print("Confusion Matrix of the given Model: \n {}".format(confusion_matrix(y_test, mod_pred)))
    print("Accuracy of the Model: \n{}".format(accuracy_score(y_test, mod_pred)))
    print("Evaluation process is done ...")
    return mod

rfc = RandomForestClassifier()
model_evaluation(rfc, X_train, y_train, X_test, y_test)

#Decision tree

dtc = DecisionTreeClassifier()
model_evaluation(dtc, X_train, y_train, X_test, y_test)

svc = SVC()
model_evaluation(svc, X_train, y_train, X_test, y_test)

#trenowanie modelu

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
plt.style.use('fivethirtyeight')
# %matplotlib inline

from xgboost import XGBClassifier, plot_importance
xgb = xgboost.XGBClassifier()
xgb = XGBClassifier(objective='binary:logistic', random_state=33, n_jobs=-1)
xgb.fit(X_train, y_train)

pip install catboost

cat = catboost.CatBoostClassifier()
model_evaluation(cat, X_train, y_train, X_test, y_test)

#Przewidywanie

xgb_predictions = xgb.predict(X_test)

def evaluation_scores(test, prediction, target_names=None):
    print('Accuracy:', np.round(metrics.accuracy_score(test, prediction), 4)) 
    print('-'*60)
    print('classification report:\n\n', metrics.classification_report(y_true=test, y_pred=prediction, target_names=target_names)) 
    
    classes = [0, 1]
    total_classes = len(classes)
    level_labels = [total_classes*[0], list(range(total_classes))]

    cm = metrics.confusion_matrix(y_true=test, y_pred=prediction, labels=classes)
    cm_frame = pd.DataFrame(data=cm, columns=pd.MultiIndex(levels=[['Predicted:'], classes], labels=level_labels), 
                            index=pd.MultiIndex(levels=[['Actual:'], classes], labels=level_labels))
    
    print('-'*60)
    print('Confusion matrix:\n')
    print(cm_frame)

model_evaluation(xgb, X_train, y_train, X_test, y_test)

#wizualizacja

from sklearn.tree import export_graphviz
from six import StringIO 
from IPython.display import Image  
import pydotplus
from sklearn.tree import plot_tree

wine = pd.read_csv('jakoscWina.csv')
feature_names = wine.columns[:10]
target_names = wine['quality'].unique().tolist()

from sklearn.tree import DecisionTreeClassifier as dtc
model = dtc(criterion = 'entropy', max_depth = 4)
model.fit(X_train, y_train)

pred_model = model.predict(X_test)

plot_tree(model)

plt.savefig('tree_visualization.png')