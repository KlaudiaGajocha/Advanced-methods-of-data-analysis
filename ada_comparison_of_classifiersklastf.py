# -*- coding: utf-8 -*-
"""ADA - Comparison of classifiersklastf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_bdQPsd2zyFPTPbuS3wCHBIO7q9jVeg2
"""

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn  as sns

dataset = pd.read_csv('mushrooms.csv')

dataset.head()

"""Funkcja head() służy do pobrania pierwszych n wierszy. Przydaje się do szybkiego testowania, czy Twój obiekt ma w sobie odpowiedni typ danych."""

dataset.corr()

"""Funkcja corr() pokazuje korelacje pomiędzy poszczególnymi danymi """

cap_colors = dataset['cap-color'].value_counts()
m_height = cap_colors.values.tolist() 
#Podaje wartości liczbowe
cap_colors.axes 
#Zapewnia etykiety wierszy
cap_color_labels = cap_colors.axes[0].tolist() 
#Konwertuje obiekt indeksu na listę


ind = np.arange(10)  # x lokalizacji dla grup
width = 0.7        
colors = ['#DEB887','#778899','#DC143C','#FFFF99','#f8f8ff','#F0DC82','#FF69B4','#D22D1E','#C000C5','g']
fig, ax = plt.subplots(figsize=(10,7))
mushroom_bars = ax.bar(ind, m_height , width, color=colors)

#Dodaje tekst na etykiety, tytuł i znaczniki osiax.set_xlabel("Cap Color",fontsize=20)
ax.set_ylabel('Quantity',fontsize=20)
ax.set_title('Mushroom Cap Color Quantity',fontsize=22)
ax.set_xticks(ind)
ax.set_xticklabels(('brown', 'gray','red','yellow','white','buff','pink','cinnamon','purple','green'),
                  fontsize = 12)

#Automatycznie oznacza liczbę grzybów dla każdego koloru paska.
def autolabel(rects,fontsize=14):
    """
    Attach a text label above each bar displaying its height
    """
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., 1*height,'%d' % int(height),
                ha='center', va='bottom',fontsize=fontsize)
autolabel(mushroom_bars)        
plt.show()

poisonous_cc = [] #Lista trujących kolorów
edible_cc = []    #Lista jadalnych kolorów grzybów
for capColor in cap_color_labels:
    size = len(dataset[dataset['cap-color'] == capColor].index)
    edibles = len(dataset[(dataset['cap-color'] == capColor) & (dataset['class'] == 'e')].index)
    edible_cc.append(edibles)
    poisonous_cc.append(size-edibles)
                        
width = 0.40
fig, ax = plt.subplots(figsize=(12,7))
edible_bars = ax.bar(ind, edible_cc , width, color='#ADFF2F')
poison_bars = ax.bar(ind+width, poisonous_cc , width, color='#DA70D6')

ax.set_xlabel("Cap Color",fontsize=20)
ax.set_ylabel('Quantity',fontsize=20)
ax.set_title('Edible and Poisonous Mushrooms Based on Cap Color',fontsize=22)
ax.set_xticks(ind + width / 2) 
ax.set_xticklabels(('brown', 'gray','red','yellow','white','buff','pink','cinnamon','purple','green'),
                  fontsize = 12)
ax.legend((edible_bars,poison_bars),('edible','poisonous'),fontsize=17)
autolabel(edible_bars, 10)
autolabel(poison_bars, 10)
plt.show()
print(edible_cc)
print(poisonous_cc)

#diagramy pokazująze podział cech grzybów na klasy 
M_cols = dataset.columns.to_list()
for col in M_cols[1:]:
    plt.figure(figsize=(10,4))
    sns.countplot(x=col , data=dataset ,palette='icefire')
    plt.title(col, fontsize=14)
    plt.show()
    print("% of total:")
    print(round((dataset[col].value_counts()/dataset.shape[0]),4)*100)

"""cap-shape - Większość grzybów w  zbiorze danych ma wypukłą (45%) lub płaską (38,8%) czapkę

cap-surface - Najczęściej powierzchnia nasadki jest łuszcząca się (39,93%) obok bez rowków (0,05%)

kolor kapelusza - Ponad połowa grzybów w zestawie danych ma kolor brązowy (28,11%) lub szary (22,65%), jest też 8 innych kolorów, które pojawiają się rzadziej.

siniaki - Większość grzybów w  zbiorze danych nie jest posiniaczonych (58,44%)

zapach - Najczęstsze zapachy (ponad 70%) to albo brak zapachu (43,43%) albo nieprzyjemny zapach (26,59%).

przywiązanie skrzelowe - Prawie całe  dane mają wolne przywiązanie skrzelowe (około 97,5%), więc ta kolumna jest prawie bezużyteczna w  analizie.

odstępy między blaszkami — ponad 85% grzybów w  zestawie danych ma bliskie odstępy między blaszkami, więc nie ma to wpływu na analizę.

wielkość gilzy - Większość grzybów w  zestawie danych ma szerokie (69,08%) rozmiary gil, inne są wąskie.

kolor skrzeli - Ponad połowa grzybów w  zestawie danych ma wypolerowany (21,27%), różowy (18,37%) lub biały (22,65%) kolor skrzeli, jest też 9 innych kolorów, które pojawiają się rzadziej.

kształt łodygi - Większość grzybów w  zestawie danych zwęża się (56,72%), inne się powiększają.

korzeń szypułkowy — najczęstsze korzenie szypułkowe (ponad 75%) są albo bulwiaste (46,48%) albo jedwabiste (30,53%).

łodyga-powierzchnia nad pierścieniem - Najczęściej powierzchnia łodygi nad pierścieniem (ponad 90%) jest gładka (63,71%) lub jedwabista (29,2%).

łodyga-powierzchnia-poniżej-pierścienia - Najczęściej powierzchnia łodygi pod pierścieniem (prawie 90%) jest albo gładka (60,76%) albo jedwabista (28,36%) prawie bez różnicy w stosunku do szypułki-powierzchnia-nad-pierścieniem.

kolor-szypułki-nad-pierścieniem - Ponad połowa grzybów w  zbiorze danych ma biały (54,95%) kolor łodygi, jest też 8 innych kolorów, które pojawiają się rzadziej.

 kolor-szypułki-pod-pierścieniem - Ponad połowa grzybów w  zestawie danych ma biały (53,96%) kolor szypułki, jest też 8 innych kolorów, które pojawiają się rzadziej, prawie nie różniąc się od koloru szypułki-nad-pierścieniem.

rodzaj welonu - Wszystkie rodzaje welonów są częściowe, więc ta kolumna jest prawie bezużyteczna w analizie.
  
kolor welonu - Prawie cały kolor welonu grzyba jest biały (97,54%), więc ta kolumna jest w analizie prawie bezużyteczna.

ring-number - Prawie cały numer pierścienia grzyba to jeden (92,17%), więc ta kolumna jest prawie bezużyteczna w analizie.

typ pierścieniowy - Ponad 98% grzybów w  zbiorze danych ma wisiorek (48,84%), zanikający (34,17%) lub duży (15,95%) pierścień.

kolor odcisku zarodników - najczęściej występującymi kolorami są biały (29,39%), brązowy (24,22%), czarny (23,04%) i czekoladowy (20,09%), pozostałe kolory są nieistotne.

populacja - Najczęstsze populacje (ponad 70%) to albo kilka (49,73%) albo samotne (21,07%).

siedlisko - Najczęstsze siedliska (ponad 65%) to lasy (38,75%) lub trawy (26,44%).
"""

dataset.isnull().sum().sum()

"""nie ma wartości zerowych """

dataset['class'].unique()

"""Kolumna klasy jest celem i ma dwie klasyfikacje, które opisują, czy grzyb jest trujący czy jadalny. W klasie kolumna trująca to p, a jadalna to e."""

dataset.info()

"""Wszystkie cechy są kategoryczne i nie ma brakujących wartości"""

dataset.shape

"""Liczba wierszy i kolumn"""

sns.histplot(dataset['class'])

"""graficzny podział na klasę p i e

#Funkcje oddzielające i cel
"""

X = dataset.drop(['class'],axis=1)
y = dataset['class']

X = pd.get_dummies(X)
X.head()

"""Zmienna dummies tworzy oddzielną kolumnę dla każdej unikatowej wartości kolumny, gdzie jako LabelEncoder koduje etykiety docelowe o wartości od 0 do n_classes-1. """

X = pd.get_dummies(X)
X.head()

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
y = encoder.fit_transform(y)
print(y)

"""#Trenowanie"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

X_train.shape , X_test.shape

y_train.shape , y_test.shape

"""#Drzewo decyzyjne"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.metrics import accuracy_score

clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)
clf_gini.fit(X_train, y_train)

plt.figure(figsize=(12,8))

tree.plot_tree(clf_gini.fit(X_train, y_train))

y_pred_gini = clf_gini.predict(X_test)
y_pred_train_gini = clf_gini.predict(X_train)
y_pred_train_gini

print('Wynik dokładności modelu z kryterium indeksu giniego: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))
print('Wynik dokładności zestawu treningowego: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))

"""Drzewo decyzyjne za pomoca entropy"""

clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)
clf_en.fit(X_train, y_train)

plt.figure(figsize=(12,8))
tree.plot_tree(clf_en.fit(X_train, y_train))

y_pred_en = clf_en.predict(X_test)

y_pred_train_en = clf_en.predict(X_train)

print('Wynik dokładności modelu z entropią kryterium: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))
print('Wynik dokładności zestawu treningowego: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))

print('Training set wynik: {:.4f}'.format(clf_en.score(X_train, y_train)))
print('Test set wynik: {:.4f}'.format(clf_en.score(X_test, y_test)))

"""Confusion Matrix"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import  f1_score

cm = confusion_matrix(y_test, y_pred_en)

print('Confusion matrix\n\n', cm)

f,ax = plt.subplots(figsize=(10, 10))
sns.heatmap(cm, annot=True, linewidths=0.5,linecolor="red", fmt= '.0f',ax=ax)
plt.show()
plt.savefig('ConfusionMatrix.png')

print(classification_report(y_test, y_pred_en))

f1_score = f1_score(y_test, y_pred_en)
print("F1 Score:",f1_score)

"""Model klasyfikatora drzewa decyzyjnego wykorzystujący zarówno indeks Giniego, jak i entropię ma tylko bardzo niewielką różnicę w dokładności modelu i dokładności zbioru uczącego, więc nie ma oznak nadmiernego dopasowania.

LogisticRegression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_validate
LR = LogisticRegression()
#walidacja krzyżowa
index = ['Valid1','Valid2','Valid3','Valid4','Valid5']
scoring = ['accuracy','precision', 'recall','f1']
LR_report = pd.DataFrame(cross_validate(LR, X, y, scoring=scoring))
#uzyskanie wymaganych metod punktacji
LR_report = LR_report.iloc[:,2:]


LR_report.columns = scoring
# nazwanie indeksu
LR_report.index = index

#finale rezultaty
model_1 = []

#report klasyfikacji 
print("Clasification report:\n")
print(LR_report,"\n\n")
print("Mean Scores:\n")
for score in scoring:
    print(f"\t{score}: {round(LR_report[score].mean(),4)}")
    model_1.append(round(LR_report[score].mean(),4))

"""mała różnica wyników porównując z drzewem decyzyjnym, jednak trochę mniejsza dokładność, precyzja, recall i f1 """

#przykładowa prognoza dla pliku csv
LR.fit(X_train, y_train)#training
y_predict = LR.predict(X_test)#predicting
Logistic_Regression=pd.DataFrame({'y_test':y_test,'prediction':y_predict})#do porównania 
Logistic_Regression.to_csv("Logistic Regression.csv")

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier()
#walidacja krzyżowa
RF_report = pd.DataFrame(cross_validate(RF, X, y, scoring=scoring))
#uzyskanie wymaganych metod punktacji
RF_report = RF_report.iloc[:,2:]
RF_report.columns = scoring
RF_report.index = index

#Zebranie finanlych wyników
model_3 = []

#raport klasyfikacji 
print("Clasification report:\n")
print(RF_report,"\n\n")
print("Mean Scores:\n")
for score in scoring:
    print(f"\t{score}: {round(RF_report[score].mean(),4)}")
    model_3.append(round(RF_report[score].mean(),4))

"""Random forest ma również zbliżone wyniki do poprzedmich klasyfikatorów, ale odrobinę mniejsza dokładność od drzewa decyzyjnego, a większa od logisticregression (LR), predycja odrobinę większa od LR, ale większa od drzewa decyzyjnego, recall wynik mniejszy, jak również f1 od poprzednich klasyfikatorów

XGBoost
"""

from xgboost import XGBClassifier
XGB = XGBClassifier()
#walidacja krzyżowa
XGB_report = pd.DataFrame(cross_validate(XGB, X, y, scoring=scoring))
#uzyskanie wymaganych metod punktacji
XGB_report = XGB_report.iloc[:,2:]
XGB_report.columns = scoring
XGB_report.index = index

#Zebranie finanlych wyników
model_4 = []

#Raport klasyfikacji 
print("Clasification report:\n")
print(XGB_report,"\n\n")
print("Mean Scores:\n")
for score in scoring:
    print(f"\t{score}: {round(XGB_report[score].mean(),4)}")
    model_4.append(round(XGB_report[score].mean(),4))

"""XGBoost ma najlepsze wyniki - najlepsza poprawność, precyzja, recall i f1 test.

#Podsumowanie 
W uczeniu maszynowym klasyfikator to algorytm, który automatycznie sortuje lub kategoryzuje dane na jedną lub więcej „klas”. Cele, etykiety i kategorie to terminy używane do opisu klas.

Regresja logistyczna to metoda przewidywania jakościowej zmiennej zależnej na podstawie zestawu niezależnych czynników.

Funkcja logistyczna służy do opisania prawdopodobieństwa prawdopodobnych wyników pojedynczego badania w tej technice. Regresja logistyczna
jest szczególnie dobra do ustalenia, jak wiele niezależnych czynników wpływa na pojedynczą zmienną wynikową.

Drzewa decyzyjne (DT) to nieparametryczna nadzorowana metoda uczenia wykorzystywana do klasyfikacji i regresji. Celem jest stworzenie modelu, który przewiduje wartość zmiennej docelowej, ucząc się prostych reguł decyzyjnych wywnioskowanych z cech danych. Drzewo może być postrzegane jako odcinkowo stałe przybliżenie.

Las losowy, tak jak drzewo decyzyjne to nadzorowane podejście do uczenia stosowane w uczeniu maszynowym do klasyfikacji i regresji. Jest to klasyfikator, który uśrednia wyniki wielu drzew decyzyjnych zastosowanych do różnych podzbiorów zestawu danych w celu poprawy przewidywanej dokładności zestawu danych.

Jest również znany jako metaestymator, ponieważ dopasowuje wiele drzew decyzyjnych do różnych podpróbek zbiorów danych i wykorzystuje średnią do zwiększenia dokładności prognozy modelu i zapobiegania nadmiernemu dopasowaniu. Rozmiar podpróbki jest zawsze taki sam jak rozmiar oryginalnej próbki wejściowej, ale próbki są generowane przy użyciu zamiany. Tworzy „las” z kolekcji drzew decyzyjnych, które są często szkolone przy użyciu metody bagging. Główną ideą podejścia baggingowego jest to, że łączy wiele modeli uczenia się i poprawia efekt końcowy. Zamiast polegać na jednym drzewie decyzyjnym, losowy las zbiera prognozy z każdego drzewa i przewiduje ostateczny wynik na podstawie większości głosów.

XGBoost to implementacja drzew decyzyjnych z gradientem, zaprojektowana z myślą o szybkości i wydajności, która jest dominującym konkurencyjnym uczeniem maszynowym. Jest to klasyfikator, który w przeprowadzonym badaniu pokazał najlepsze wyniki.
"""